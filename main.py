from agents import Agent, Runner, AsyncOpenAI, OpenAIChatCompletionsModel, ModelSettings, TResponseInputItem
import asyncio
from agents import set_default_openai_client, set_tracing_disabled
import mlflow.openai
from openai.types.responses import ResponseTextDeltaEvent
from dotenv import load_dotenv
load_dotenv()
import os
import chainlit as cl
from agents.mcp import MCPServer, MCPServerStdio
import mlflow

api_key = os.getenv("GEMINI_API_KEY")
if not api_key:
    print("Warning: LLM api-key not found in environment variables.")

url = os.getenv("GEMINI_URL")
if not url:
    print("Warning: LLM api-key url not found in environment variables.")

# Get API key from environment variables
firecrawl_api_key = os.getenv("FIRECRAWL_API_KEY")
if not firecrawl_api_key:
    print("Warning: FIRECRAWL_API_KEY not found in environment variables. Using fallback method.")


# mlflow.openai.autolog()
# mlflow.set_tracking_uri("http://localhost:5000")
# mlflow.set_experiment("OpenAI")

@cl.password_auth_callback
def password_auth(username: str, password: str):
    if(username, password) == ("admin", "admin"):
        return cl.User(
            identifier="admin",
            display_name="vayne",
            metadata={"role": "admin", "provider": "credentials"}
        )
    return None


@cl.on_message
async def main(message: cl.Message):
    custom_client = AsyncOpenAI(
        api_key=api_key,
        base_url=url

    )
    
    set_default_openai_client(custom_client)
    set_tracing_disabled(True)
    res=cl.Message(content="")
    input_items = cl.user_session.get("input_items", [])
    user_input = message.content
    input_items.append({"content": user_input, "role": "user"})
    try:
        # ä½¿ç”¨ OpenAIChatCompletionsModel é…ç½®ä»£ç†
        async def run(mcp_server1: MCPServer, mcp_server2: MCPServer):
            agent = Agent(
                name="Assistant",
                instructions="you use tools to answer questions",
                model=OpenAIChatCompletionsModel(
                    model="gemini-2.0-flash",
                    openai_client=custom_client,
                ),
                model_settings=ModelSettings(temperature=0.7),
                mcp_servers=[mcp_server1, mcp_server2],
            )
            
            result = Runner.run_streamed(
                starting_agent=agent,
                input=input_items)
            
            async for event in result.stream_events():
                if event.type == "raw_response_event" and isinstance(event.data, ResponseTextDeltaEvent):
                    delta = event.data.delta
                    if delta:
                        await res.stream_token(delta)
                        await res.send()                
            cl.user_session.set("input_items", result.to_input_list())
            

        async with MCPServerStdio(
            name="SequentialThinking MCP Server, via npx",
            params={
                "command": "npx",
                "args": ["-y", "@modelcontextprotocol/server-sequential-thinking"]
            }
        ) as sequential_thinking_server, MCPServerStdio(
            name="Firecrawl MCP Server",
            params={
                "command": "npx",
                "args": ["-y", "firecrawl-mcp"],
                "env": {
                    "FIRECRAWL_API_KEY": firecrawl_api_key
                }
            }
        ) as firecrawl_server:
            await run(sequential_thinking_server, firecrawl_server)
              
    except Exception as e:
        print(f"å‘ç”Ÿé”™è¯¯: {str(e)}")
        
    
'''
ğŸ“˜ Chainlit é¡¹ç›®ä¸­ input_items ç®¡ç†çŸ¥è¯†ç‚¹æ€»ç»“ï¼ˆæ–‡æœ¬ç‰ˆï¼‰
ä¸€ã€å˜é‡ä½œç”¨åŸŸ & åˆå§‹åŒ–ä½ç½®
input_items æ˜¯ç”¨äºä¿å­˜ç”¨æˆ·-åŠ©æ‰‹å¯¹è¯çš„ä¸Šä¸‹æ–‡è®°å½•ã€‚

å¦‚æœä½ åœ¨å‡½æ•°å†…éƒ¨åˆå§‹åŒ–å®ƒï¼ˆå¦‚ input_items = []ï¼‰ï¼Œæ¯æ¬¡å‡½æ•°è°ƒç”¨éƒ½ä¼šé‡ç½®å®ƒï¼Œæ— æ³•ä¿ç•™ä¸Šä¸‹æ–‡ã€‚

æ‰€ä»¥ï¼š

å…¨å±€å˜é‡æ–¹å¼ï¼šåˆå§‹åŒ–æ”¾åœ¨å‡½æ•°å¤–ï¼ˆé€‚åˆå•ç”¨æˆ·æµ‹è¯•ï¼‰

æ¨èæ–¹å¼ï¼šä½¿ç”¨ cl.user_session å­˜å‚¨ä¸Šä¸‹æ–‡ï¼ˆæ”¯æŒå¤šç”¨æˆ·ï¼‰

äºŒã€æ˜¯å¦éœ€è¦ä½¿ç”¨ global
å½“ä½ åœ¨å‡½æ•°ä¸­ä¿®æ”¹å‡½æ•°å¤–å®šä¹‰çš„å˜é‡ï¼ˆå¦‚ input_items.append(...)ï¼‰æ—¶ï¼š

å¿…é¡»å†™ global input_itemsï¼Œå¦åˆ™ Python ä¼šå½“ä½œå±€éƒ¨å˜é‡å¤„ç†ï¼Œå¯¼è‡´å‡ºé”™æˆ–æ— æ•ˆã€‚

å¦‚æœä½ å®Œå…¨åœ¨å‡½æ•°å†…éƒ¨ä½¿ç”¨å±€éƒ¨å˜é‡ï¼Œåˆ™æ— éœ€ä½¿ç”¨ globalã€‚

ä¸‰ã€Chainlit çš„ @cl.on_message å·¥ä½œæœºåˆ¶
@cl.on_message æ˜¯ Chainlit çš„äº‹ä»¶å›è°ƒè£…é¥°å™¨ã€‚

æ¯å½“ç”¨æˆ·å‘æ¥æ–°æ¶ˆæ¯ï¼Œä¸»å‡½æ•°ï¼ˆå¦‚ main(message)ï¼‰å°±ä¼šé‡æ–°æ‰§è¡Œã€‚

æ‰€ä»¥ä¸è¦åœ¨å‡½æ•°å†…ä½¿ç”¨ input_items = []ï¼Œå¦åˆ™æ¯æ¬¡éƒ½é‡ç½®å¯¹è¯ä¸Šä¸‹æ–‡ã€‚

å››ã€ä¿ç•™ä¸Šä¸‹æ–‡çš„ä¸¤ç§æ–¹æ³•
æ–¹æ³•ä¸€ï¼šä½¿ç”¨å…¨å±€å˜é‡ï¼ˆé€‚åˆæœ¬åœ°æµ‹è¯•ï¼‰
python
Copy
Edit
input_items = []

@cl.on_message
async def main(message):
    global input_items
    input_items.append({"role": "user", "content": message.content})
æ–¹æ³•äºŒï¼šä½¿ç”¨ cl.user_sessionï¼ˆæ¨èï¼Œæ”¯æŒå¤šç”¨æˆ·ï¼‰
python
Copy
Edit
@cl.on_message
async def main(message):
    input_items = cl.user_session.get("input_items", [])
    input_items.append({"role": "user", "content": message.content})
    cl.user_session.set("input_items", input_items)
äº”ã€input_items çš„ç±»å‹æ³¨è§£ï¼ˆTResponseInputItemï¼‰
åŸä»£ç å†™æ³•ä¸­ä½¿ç”¨äº† input_items: list[TResponseInputItem]ã€‚

å¦‚æœ TResponseInputItem æ²¡æœ‰å®šä¹‰æˆ–å¯¼å…¥ï¼Œä¼šå¯¼è‡´ç±»å‹é”™è¯¯ã€‚

è§£å†³æ–¹æ¡ˆï¼š
ç®€å•æ–¹å¼ï¼ˆä¸å†™ç±»å‹ï¼‰ï¼š
python
Copy
Edit
input_items = cl.user_session.get("input_items", [])
æ ‡å‡†æ–¹å¼ï¼ˆè‡ªå®šä¹‰ç±»å‹ï¼‰ï¼š
python
Copy
Edit
from typing import TypedDict

class TResponseInputItem(TypedDict):
    role: str
    content: str

input_items: list[TResponseInputItem] = cl.user_session.get("input_items", [])
å…­ã€æ€»ç»“å»ºè®®
åŠŸèƒ½	æ¨èåšæ³•
å•ç”¨æˆ·è°ƒè¯•	ä½¿ç”¨å…¨å±€å˜é‡
å¤šç”¨æˆ·å¯¹è¯ç®¡ç†	ä½¿ç”¨ cl.user_session
å˜é‡ä½œç”¨åŸŸ	ä¿®æ”¹å…¨å±€å˜é‡éœ€åŠ  global
å¯¹è¯ä¸Šä¸‹æ–‡æ ¼å¼	ä½¿ç”¨ {"role": "user/assistant", "content": "..."}
ç±»å‹æ ‡æ³¨	å¯è‡ªå®šä¹‰ TypedDict ç±»å‹ç”¨äºæç¤º

ä¸ƒã€æ¨èä¸»å‡½æ•°ç»“æ„ç¤ºä¾‹
python
Copy
Edit
@cl.on_message
async def main(message: cl.Message):
    # è·å–å½“å‰ç”¨æˆ·ä¼šè¯ä¸Šä¸‹æ–‡
    input_items = cl.user_session.get("input_items", [])

    # æ·»åŠ ç”¨æˆ·è¾“å…¥
    input_items.append({"role": "user", "content": message.content})

    # è®¾ç½® LLM å’Œ Agent æ‰§è¡Œè¿‡ç¨‹ï¼ˆç•¥ï¼‰

    # ä¿å­˜å¯¹è¯å†å²
    cl.user_session.set("input_items", input_items)

'''        
# if __name__ == "__main__":
#     asyncio.run(main())


# external_client = AsyncOpenAI(
#     api_key="AIzaSyB9q2IHi3djY_TuVaS_euD0Z-nUy8HvbCY",
#     base_url='https://generativelanguage.googleapis.com/v1beta'
# )

# chines_agent = Agent(
#     name="Chinese agent",
#     instructions='you are speak chinise'
#     model=OpenAIChatCompletionsModel(
#         model_name="gemini-2.0-flash",
#         openai_client=external_client,

#     ),
#     model_settings=model_settings()
# )


# if __name__ == "__main__":
#     main()
